{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated distribution plot for 493\n",
      "Generated distribution plot for 276\n",
      "Generated distribution plot for 997\n",
      "Generated distribution plot for V498\n",
      "Generated distribution plot for 305\n",
      "Generated distribution plot for 300\n",
      "Generated distribution plot for 780\n",
      "Generated distribution plot for 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404415/942982230.py:410: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  top_words = base_attention.groupby('Word')['AttentionWeight'].mean().nlargest(top_n).index\n",
      "/tmp/ipykernel_404415/942982230.py:416: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  word_attention = df[df['Word'].isin(top_words)].groupby('Word')['AttentionWeight'].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated attention heatmap\n",
      "Generated analysis report\n",
      "Analysis completed. Results saved to analysis_result\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class AttentionAnalysisResults:\n",
    "    \"\"\"Stores results from attention weight analysis\"\"\"\n",
    "    val_type_effects: pd.DataFrame\n",
    "    val_class_effects: pd.DataFrame\n",
    "    significance_tests: pd.DataFrame\n",
    "    top_attention_shifts: pd.DataFrame\n",
    "\n",
    "@dataclass\n",
    "class DiagnosisTestResults:\n",
    "    \"\"\"Stores results from diagnosis statistical testing\"\"\"\n",
    "    test_results: pd.DataFrame\n",
    "    significant_diagnoses: List[str]\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    def __init__(self, results_dir: str):\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.diagnosis_dfs = {}\n",
    "        self.attention_dfs = {}\n",
    "        self.notes_dfs = {}\n",
    "        \n",
    "        self.VALS_TYPES = {\n",
    "            \"pejorative\": frozenset([\"non_compliant\", \"uncooperative\", \"resistant\", \"difficult\"]),\n",
    "            \"laudatory\": frozenset([\"compliant\", \"cooperative\", \"pleasant\", \"respectful\"]),\n",
    "            \"neutralval\": frozenset(['neutral'])\n",
    "        }\n",
    "        self._load_results()\n",
    "    \n",
    "    def _load_results(self):\n",
    "        \"\"\"Load all results files from the directory\"\"\"\n",
    "        self._load_csv_files(\"_diagnosis.csv\", self.diagnosis_dfs)\n",
    "        self._load_csv_files(\"_attention.csv\", self.attention_dfs)\n",
    "        self._load_csv_files(\"_clinical_notes.csv\", self.notes_dfs)\n",
    "\n",
    "    def _load_csv_files(self, file_pattern: str, dfs_dict: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Helper function to load CSV files matching the pattern\"\"\"\n",
    "        for file in self.results_dir.glob(f\"*{file_pattern}\"):\n",
    "            name = file.stem.split('_')[0]\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            if 'AttentionWeight' in df.columns:\n",
    "                df['AttentionWeight'] = pd.to_numeric(df['AttentionWeight'], errors='coerce')\n",
    "            \n",
    "            categorical_cols = ['Valence', 'Val_class', 'Word']\n",
    "            for col in categorical_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('category')\n",
    "            \n",
    "            dfs_dict[name] = df\n",
    "    \n",
    "    def analyze_attention_patterns(self) -> AttentionAnalysisResults:\n",
    "        \"\"\"Analyze attention weight changes across valences and word types\"\"\"\n",
    "        combined_attention = pd.concat(self.attention_dfs.values(), ignore_index=True)\n",
    "        \n",
    "        mask_neutralize = combined_attention[\"Val_class\"] == \"neutralize\"\n",
    "        baseline = (combined_attention[mask_neutralize]\n",
    "                   .groupby(\"Word\", observed=True)[\"AttentionWeight\"]\n",
    "                   .mean()\n",
    "                   .reset_index())\n",
    "        \n",
    "        baseline_dict = dict(zip(baseline[\"Word\"], baseline[\"AttentionWeight\"]))\n",
    "        baseline_words = set(baseline[\"Word\"])\n",
    "        \n",
    "        val_effects = []\n",
    "        \n",
    "        for val_class, vals in self.VALS_TYPES.items():\n",
    "            for val in vals:\n",
    "                mask = combined_attention[\"Valence\"].str.contains(val, case=False, na=False)\n",
    "                filter_table = combined_attention[mask]\n",
    "                common_words = list(set(filter_table[\"Word\"]) & baseline_words)\n",
    "                \n",
    "                if not common_words:\n",
    "                    continue\n",
    "                \n",
    "                val_data = filter_table[filter_table[\"Word\"].isin(common_words)]\n",
    "                \n",
    "                if len(val_data) == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    baseline_values = np.array([baseline_dict[word] for word in val_data[\"Word\"]])\n",
    "                    effect = self._calculate_effect_size(\n",
    "                        val_data[\"AttentionWeight\"].values,\n",
    "                        baseline_values\n",
    "                    )\n",
    "                    val_effects.append({\n",
    "                        \"valence_type\": val,\n",
    "                        \"val_class\": val_class,\n",
    "                        \"effect_size\": effect\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "        vals_types_effects = pd.DataFrame(val_effects)\n",
    "        \n",
    "        class_effects = (vals_types_effects\n",
    "                        .groupby(\"val_class\", observed=True)[\"effect_size\"]\n",
    "                        .agg([\"mean\", \"std\", \"count\"])\n",
    "                        .reset_index())\n",
    "        \n",
    "        significance_tests = self._run_significance_tests(combined_attention)\n",
    "        top_shifts = self._analyze_attention_shifts(combined_attention, baseline[\"AttentionWeight\"])\n",
    "\n",
    "        return AttentionAnalysisResults(\n",
    "            val_type_effects=vals_types_effects,\n",
    "            val_class_effects=class_effects,\n",
    "            significance_tests=significance_tests,\n",
    "            top_attention_shifts=top_shifts\n",
    "        )\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_effect_size(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Cohen's d effect size between two groups\"\"\"\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        if n1 < 2 or n2 < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        var1 = np.var(group1, ddof=1)\n",
    "        var2 = np.var(group2, ddof=1)\n",
    "        pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "        \n",
    "        if pooled_se == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return abs((np.mean(group1) - np.mean(group2))) / pooled_se\n",
    "    \n",
    "    def _run_significance_tests(self, attention_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run statistical tests on attention weight differences\"\"\"\n",
    "        neutralize_mask = attention_data.Val_class == \"neutralize\"\n",
    "        neutralize_weights = attention_data[neutralize_mask][\"AttentionWeight\"].values\n",
    "        \n",
    "        results = []\n",
    "        classes = attention_data.Val_class.unique()\n",
    "        \n",
    "        for val_class in classes:\n",
    "            if val_class == \"neutralize\":\n",
    "                continue\n",
    "            \n",
    "            class_mask = attention_data.Val_class == val_class\n",
    "            class_weights = attention_data[class_mask][\"AttentionWeight\"].values\n",
    "            \n",
    "            t_stat, p_val = stats.ttest_ind(class_weights, neutralize_weights)\n",
    "            \n",
    "            results.append({\n",
    "                \"valence_class\": val_class,\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_val\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df[\"adjusted_p\"] = multipletests(results_df[\"p_value\"], method=\"fdr_bh\")[1]\n",
    "        \n",
    "        for col in [\"p_value\", \"adjusted_p\"]:\n",
    "            results_df[col] = results_df[col].round(4)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    def _analyze_attention_shifts(self, attention_data: pd.DataFrame, baseline: pd.Series, top_n: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Identify words with largest attention weight changes\"\"\"\n",
    "        shifts = (attention_data\n",
    "                 .groupby(['Word', 'Val_class'], observed=True)['AttentionWeight']\n",
    "                 .mean()\n",
    "                 .unstack()\n",
    "                 .fillna(0))\n",
    "\n",
    "        neutralize_mask = ~shifts['neutralize'].isna()\n",
    "        valid_words = shifts[neutralize_mask].index\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        for val_class in shifts.columns:\n",
    "            if val_class == 'neutralize':\n",
    "                continue\n",
    "            \n",
    "            shift_data = pd.DataFrame({\n",
    "                'Word': valid_words,\n",
    "                'Neutralize_Weight': shifts.loc[valid_words, 'neutralize'],\n",
    "                'Shifted_Weight': shifts.loc[valid_words, val_class],\n",
    "                'Valence_Class': val_class\n",
    "            })\n",
    "            \n",
    "            shift_data['Absolute_Shift'] = abs(shift_data['Shifted_Weight'] - shift_data['Neutralize_Weight'])\n",
    "            shift_data['Percentage_Change'] = (\n",
    "                (shift_data['Shifted_Weight'] - shift_data['Neutralize_Weight']) / \n",
    "                shift_data['Neutralize_Weight'] * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            top_shifts = (shift_data\n",
    "                         .nlargest(top_n, 'Absolute_Shift')\n",
    "                         .reset_index(drop=True))\n",
    "            \n",
    "            result_dfs.append(top_shifts)\n",
    "        \n",
    "        final_shifts = pd.concat(result_dfs, ignore_index=True)\n",
    "        \n",
    "        formatted_shifts = (final_shifts\n",
    "            .assign(\n",
    "                Neutral_Weight=lambda x: x['Neutralize_Weight'].round(4),\n",
    "                Shifted_Weight=lambda x: x['Shifted_Weight'].round(4),\n",
    "                Absolute_Shift=lambda x: x['Absolute_Shift'].round(4)\n",
    "            )\n",
    "            .sort_values(['Valence_Class', 'Absolute_Shift'], ascending=[True, False])\n",
    "            .reset_index(drop=True))\n",
    "        \n",
    "        formatted_shifts['Rank'] = (formatted_shifts\n",
    "            .groupby('Valence_Class')\n",
    "            .cumcount() + 1)\n",
    "        \n",
    "        return formatted_shifts[[\n",
    "            'Rank',\n",
    "            'Valence_Class', \n",
    "            'Word',\n",
    "            'Neutral_Weight',\n",
    "            'Shifted_Weight', \n",
    "            'Absolute_Shift',\n",
    "            'Percentage_Change'\n",
    "        ]]\n",
    "    \n",
    "\n",
    "    def test_diagnosis_significance(self) -> DiagnosisTestResults:\n",
    "        \"\"\"\n",
    "        Perform statistical tests to compare diagnosis predictions across different valences\n",
    "        Returns both test results and list of significant diagnoses\n",
    "        \"\"\"\n",
    "        diagnoses = self.get_available_diagnoses()\n",
    "        test_results = []\n",
    "        \n",
    "        for diagnosis in diagnoses:\n",
    "            # Get neutral predictions as baseline\n",
    "            neutralize_data = None\n",
    "            for valence, df in self.diagnosis_dfs.items():\n",
    "                if 'neutralize' in valence.lower() and diagnosis in df.columns:\n",
    "                    neutralize_data = pd.to_numeric(df[diagnosis], errors='coerce').dropna()\n",
    "                    break\n",
    "            \n",
    "            if neutralize_data is None or len(neutralize_data) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Compare each valence against baseline\n",
    "            for valence, df in self.diagnosis_dfs.items():\n",
    "                if 'neutraize' in valence.lower() or diagnosis not in df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                test_data = pd.to_numeric(df[diagnosis], errors='coerce').dropna()\n",
    "                if len(test_data) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # Perform t-test\n",
    "                t_stat, p_val = stats.ttest_ind(neutralize_data, test_data)\n",
    "                effect_size = self._calculate_effect_size(neutralize_data.values, test_data.values)\n",
    "                \n",
    "                test_results.append({\n",
    "                    'diagnosis': diagnosis,\n",
    "                    'valence': valence,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_val,\n",
    "                    'effect_size': effect_size\n",
    "                })\n",
    "        \n",
    "        if not test_results:\n",
    "            return DiagnosisTestResults(pd.DataFrame(), [])\n",
    "            \n",
    "        # Create results DataFrame and adjust p-values\n",
    "        results_df = pd.DataFrame(test_results)\n",
    "        results_df['adjusted_p'] = multipletests(results_df['p_value'], method='fdr_bh')[1]\n",
    "        \n",
    "        # Get list of significant diagnoses\n",
    "        alpha = 0.05\n",
    "        significant_diagnoses = (results_df[results_df['adjusted_p'] < alpha]\n",
    "                               ['diagnosis'].unique().tolist())\n",
    "        \n",
    "        # Round numeric columns\n",
    "        for col in ['t_statistic', 'p_value', 'adjusted_p', 'effect_size']:\n",
    "            results_df[col] = results_df[col].round(4)\n",
    "            \n",
    "        return DiagnosisTestResults(results_df, significant_diagnoses)\n",
    "    \n",
    "\n",
    "    def get_distribution_statistics(self, diagnosis: str) -> pd.DataFrame:\n",
    "        \"\"\"Calculate statistical summary for prediction distributions\"\"\"\n",
    "        stats_data = []\n",
    "\n",
    "        for valence, df in self.diagnosis_dfs.items():\n",
    "            if diagnosis not in df.columns:\n",
    "                continue\n",
    "\n",
    "            numeric_data = pd.to_numeric(df[diagnosis], errors='coerce')\n",
    "            df_cleaned = numeric_data.dropna()\n",
    "            \n",
    "            if len(df_cleaned) > 0:\n",
    "                stats_dict = {\n",
    "                    'valence': valence,\n",
    "                    'count': len(df_cleaned),\n",
    "                    'mean': df_cleaned.mean(),\n",
    "                    'std': df_cleaned.std(),\n",
    "                    'min': df_cleaned.min(),\n",
    "                    'max': df_cleaned.max(),\n",
    "                    'median': df_cleaned.median()\n",
    "                }\n",
    "                stats_data.append(stats_dict)\n",
    "\n",
    "        return pd.DataFrame(stats_data) if stats_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "    def plot_prediction_distribution(self, diagnosis: str, save_path: str):\n",
    "        \"\"\"\n",
    "        Plots the distribution of predictions for a specific diagnosis across different valences.\n",
    "        \n",
    "        Args:\n",
    "            diagnosis (str): The diagnosis code to plot distributions for\n",
    "            save_path (str): Path to save the generated plot\n",
    "        \"\"\"\n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Get statistical test results for this diagnosis\n",
    "        test_results = self.test_diagnosis_significance()\n",
    "        sig_results = test_results.test_results[\n",
    "            test_results.test_results['diagnosis'] == diagnosis\n",
    "        ]\n",
    "        \n",
    "        # Plot distribution for each valence\n",
    "        legend_elements = []\n",
    "        for valence, df in self.diagnosis_dfs.items():\n",
    "            if diagnosis not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Convert to numeric and drop NaN values\n",
    "            values = pd.to_numeric(df[diagnosis], errors='coerce').dropna()\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Get p-value for this valence if available\n",
    "            p_val = 1.0\n",
    "            if not sig_results.empty:\n",
    "                val_result = sig_results[sig_results['valence'] == valence]\n",
    "                if not val_result.empty:\n",
    "                    p_val = val_result['adjusted_p'].iloc[0]\n",
    "            \n",
    "            # Create label with p-value for significant results\n",
    "            label = valence\n",
    "            if p_val < 0.05:\n",
    "                label = f'{valence} (p={p_val:.4f})'\n",
    "            \n",
    "            # Plot the distribution\n",
    "            sns.kdeplot(\n",
    "                data=values,\n",
    "                label=label,\n",
    "                common_norm=False,\n",
    "                fill=True,\n",
    "                alpha=0.3\n",
    "            )\n",
    "            \n",
    "        # Customize plot\n",
    "        plt.title(f'Prediction Distribution for {diagnosis}')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend(title='Valence Categories')\n",
    "        \n",
    "        # Add distribution statistics as text\n",
    "        stats_df = self.get_distribution_statistics(diagnosis)\n",
    "        if not stats_df.empty:\n",
    "            stats_text = \"Distribution Statistics:\\n\"\n",
    "            for _, row in stats_df.iterrows():\n",
    "                stats_text += f\"\\n{row['valence']}:\\n\"\n",
    "                stats_text += f\"mean: {row['mean']:.3f}\\n\"\n",
    "                stats_text += f\"std: {row['std']:.3f}\\n\"\n",
    "            \n",
    "            plt.figtext(\n",
    "                1.02, 0.5,\n",
    "                stats_text,\n",
    "                fontsize=8,\n",
    "                va='center'\n",
    "            )\n",
    "        \n",
    "        # Save plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        return sig_results\n",
    "\n",
    "    \n",
    "    def get_available_diagnoses(self) -> List[str]:\n",
    "        \"\"\"Get list of available diagnoses\"\"\"\n",
    "        diagnoses = set()\n",
    "        for df in self.diagnosis_dfs.values():\n",
    "            exclude_cols = {'Valence', 'Val_class', 'Word', 'AttentionWeight'}\n",
    "            diagnoses.update(set(df.columns) - exclude_cols)\n",
    "        return list(diagnoses)\n",
    "    \n",
    "    def plot_attention_heatmap(self, top_n: int = 20, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot heatmap of attention weight changes\"\"\"\n",
    "        if 'neutralize' not in self.attention_dfs:\n",
    "            return\n",
    "\n",
    "        base_attention = self.attention_dfs['neutralize']\n",
    "        top_words = base_attention.groupby('Word')['AttentionWeight'].mean().nlargest(top_n).index\n",
    "\n",
    "        attention_matrix = []\n",
    "        valences = []\n",
    "\n",
    "        for valence, df in self.attention_dfs.items():\n",
    "            word_attention = df[df['Word'].isin(top_words)].groupby('Word')['AttentionWeight'].mean()\n",
    "            attention_matrix.append(word_attention)\n",
    "            valences.append(valence)\n",
    "\n",
    "        attention_df = pd.DataFrame(attention_matrix, index=valences, columns=top_words)\n",
    "\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(attention_df, cmap='RdBu_r', center=0, annot=True, fmt='.2f')\n",
    "        plt.title('Attention Weight Changes Across Valences')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Valence')\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def generate_all_visualizations(self, save_dir: str, diagnoses: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Generate visualizations for specified or significant diagnoses\n",
    "        \n",
    "        Args:\n",
    "            save_dir (str): Directory to save visualizations\n",
    "            diagnoses (Optional[List[str]]): List of diagnoses to visualize. If None, uses significant diagnoses.\n",
    "        \"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get significant diagnoses\n",
    "        test_results = self.test_diagnosis_significance()\n",
    "        significant_diagnoses = test_results.significant_diagnoses\n",
    "\n",
    "        if diagnoses is None:\n",
    "            # Use all available diagnoses that show significant effects\n",
    "            diagnoses = significant_diagnoses\n",
    "        else:\n",
    "            # Filter requested diagnoses to only include significant ones\n",
    "            diagnoses = [d for d in diagnoses if d in significant_diagnoses]\n",
    "\n",
    "        # Generate distribution plots\n",
    "        for diagnosis in diagnoses:\n",
    "            diagnosis_path = save_path / f\"prediction_dist_{diagnosis}.png\"\n",
    "            try:\n",
    "                sig_results = self.plot_prediction_distribution(\n",
    "                    diagnosis,\n",
    "                    str(diagnosis_path)\n",
    "                )\n",
    "                print(f\"Generated distribution plot for {diagnosis}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating plot for {diagnosis}: {str(e)}\")\n",
    "\n",
    "        # Generate attention heatmap\n",
    "        try:\n",
    "            heatmap_path = save_path / \"attention_heatmap.png\"\n",
    "            self.plot_attention_heatmap(save_path=str(heatmap_path))\n",
    "            print(\"Generated attention heatmap\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating attention heatmap: {str(e)}\")\n",
    "\n",
    "        # Generate summary report\n",
    "        try:\n",
    "            self._generate_summary_report(\n",
    "                save_path / \"analysis_report.md\",\n",
    "                test_results\n",
    "            )\n",
    "            print(\"Generated analysis report\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating analysis report: {str(e)}\")\n",
    "\n",
    "            \n",
    "    def _generate_summary_report(self, save_path: Path, diagnosis_tests: Optional[DiagnosisTestResults] = None):\n",
    "        \"\"\"Generate comprehensive analysis report including statistical test results\"\"\"\n",
    "        analysis = self.analyze_attention_patterns()\n",
    "        \n",
    "        with open(save_path, \"w\") as f:\n",
    "            f.write(\"# Analysis Report\\n\\n\")\n",
    "            \n",
    "            if diagnosis_tests and not diagnosis_tests.test_results.empty:\n",
    "                f.write(\"## Diagnosis Statistical Tests\\n\")\n",
    "                f.write(diagnosis_tests.test_results.to_markdown() + \"\\n\\n\")\n",
    "                \n",
    "                f.write(\"### Significant Diagnoses\\n\")\n",
    "                for diagnosis in diagnosis_tests.significant_diagnoses:\n",
    "                    f.write(f\"- {diagnosis}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Word Type Effects\\n\")\n",
    "            if not analysis.val_type_effects.empty:\n",
    "                f.write(analysis.val_type_effects.to_markdown() + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No valid word type effects found.\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Word Class Effects\\n\")\n",
    "            if not analysis.val_class_effects.empty:\n",
    "                f.write(analysis.val_class_effects.to_markdown() + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No valid word class effects found.\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Distribution Statistics by Significant Diagnosis\\n\")\n",
    "            significant_diagnoses = (diagnosis_tests.significant_diagnoses \n",
    "                                   if diagnosis_tests else self.get_available_diagnoses())\n",
    "            \n",
    "            for diagnosis in significant_diagnoses:\n",
    "                stats_df = self.get_distribution_statistics(diagnosis)\n",
    "                if not stats_df.empty:\n",
    "                    f.write(f\"\\n### {diagnosis}\\n\")\n",
    "                    f.write(stats_df.to_markdown() + \"\\n\")\n",
    "\n",
    "                    \n",
    "    def analyze_full_dataset(self, output_dir: str):\n",
    "        \"\"\"Perform comprehensive analysis of the entire dataset\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.generate_all_visualizations(str(output_path / \"visualizations\"))\n",
    "        self._generate_summary_report(output_path / \"full_analysis_report.md\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    results_dir = \"result\"\n",
    "    output_dir = \"analysis_result\"\n",
    "\n",
    "    # Initialize the ResultsAnalyzer\n",
    "    analyzer = ResultsAnalyzer(results_dir)\n",
    "\n",
    "    # Perform the analysis\n",
    "    analyzer.analyze_full_dataset(output_dir)\n",
    "\n",
    "    print(f\"Analysis completed. Results saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_404415/1782409153.py:278: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  top_words = base_attention.groupby('Word')['AttentionWeight'].mean().nlargest(top_n).index\n",
      "/tmp/ipykernel_404415/1782409153.py:284: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  word_attention = df[df['Word'].isin(top_words)].groupby('Word')['AttentionWeight'].mean()\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class AttentionAnalysisResults:\n",
    "    \"\"\"Stores results from attention weight analysis\"\"\"\n",
    "    val_type_effects: pd.DataFrame\n",
    "    val_class_effects: pd.DataFrame\n",
    "    significance_tests: pd.DataFrame\n",
    "    top_attention_shifts: pd.DataFrame\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    def __init__(self, results_dir: str):\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.diagnosis_dfs = {}\n",
    "        self.attention_dfs = {}\n",
    "        self.notes_dfs = {}\n",
    "        \n",
    "        self.VALS_TYPES = {\n",
    "            \"pejorative\": frozenset([\"non_compliant\", \"uncooperative\", \"resistant\", \"difficult\"]),\n",
    "            \"laudatory\": frozenset([\"compliant\", \"cooperative\", \"pleasant\", \"respectful\"]),\n",
    "            \"neutralval\": frozenset(['neutral'])\n",
    "        }\n",
    "        self._load_results()\n",
    "    \n",
    "    def _load_results(self):\n",
    "        \"\"\"Load all results files from the directory\"\"\"\n",
    "        self._load_csv_files(\"_diagnosis.csv\", self.diagnosis_dfs)\n",
    "        self._load_csv_files(\"_attention.csv\", self.attention_dfs)\n",
    "        self._load_csv_files(\"_clinical_notes.csv\", self.notes_dfs)\n",
    "\n",
    "    def _load_csv_files(self, file_pattern: str, dfs_dict: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Helper function to load CSV files matching the pattern\"\"\"\n",
    "        for file in self.results_dir.glob(f\"*{file_pattern}\"):\n",
    "            name = file.stem.split('_')[0]\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            if 'AttentionWeight' in df.columns:\n",
    "                df['AttentionWeight'] = pd.to_numeric(df['AttentionWeight'], errors='coerce')\n",
    "            \n",
    "            categorical_cols = ['Valence', 'Val_class', 'Word']\n",
    "            for col in categorical_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('category')\n",
    "            \n",
    "            dfs_dict[name] = df\n",
    "\n",
    "    def analyze_attention_patterns(self) -> AttentionAnalysisResults:\n",
    "        \"\"\"Analyze attention weight changes across valences and word types\"\"\"\n",
    "        combined_attention = pd.concat(self.attention_dfs.values(), ignore_index=True)\n",
    "        \n",
    "        mask_neutralize = combined_attention[\"Val_class\"] == \"neutralize\"\n",
    "        baseline = (combined_attention[mask_neutralize]\n",
    "                   .groupby(\"Word\", observed=True)[\"AttentionWeight\"]\n",
    "                   .mean()\n",
    "                   .reset_index())\n",
    "        \n",
    "        baseline_dict = dict(zip(baseline[\"Word\"], baseline[\"AttentionWeight\"]))\n",
    "        baseline_words = set(baseline[\"Word\"])\n",
    "        \n",
    "        val_effects = []\n",
    "        \n",
    "        for val_class, vals in self.VALS_TYPES.items():\n",
    "            for val in vals:\n",
    "                mask = combined_attention[\"Valence\"].str.contains(val, case=False, na=False)\n",
    "                filter_table = combined_attention[mask]\n",
    "                common_words = list(set(filter_table[\"Word\"]) & baseline_words)\n",
    "                \n",
    "                if not common_words:\n",
    "                    continue\n",
    "                \n",
    "                val_data = filter_table[filter_table[\"Word\"].isin(common_words)]\n",
    "                \n",
    "                if len(val_data) == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    baseline_values = np.array([baseline_dict[word] for word in val_data[\"Word\"]])\n",
    "                    effect = self._calculate_effect_size(\n",
    "                        val_data[\"AttentionWeight\"].values,\n",
    "                        baseline_values\n",
    "                    )\n",
    "                    val_effects.append({\n",
    "                        \"valence_type\": val,\n",
    "                        \"val_class\": val_class,\n",
    "                        \"effect_size\": effect\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "        vals_types_effects = pd.DataFrame(val_effects)\n",
    "        \n",
    "        class_effects = (vals_types_effects\n",
    "                        .groupby(\"val_class\", observed=True)[\"effect_size\"]\n",
    "                        .agg([\"mean\", \"std\", \"count\"])\n",
    "                        .reset_index())\n",
    "        \n",
    "        significance_tests = self._run_significance_tests(combined_attention)\n",
    "        top_shifts = self._analyze_attention_shifts(combined_attention, baseline[\"AttentionWeight\"])\n",
    "\n",
    "        return AttentionAnalysisResults(\n",
    "            val_type_effects=vals_types_effects,\n",
    "            val_class_effects=class_effects,\n",
    "            significance_tests=significance_tests,\n",
    "            top_attention_shifts=top_shifts\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_effect_size(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Cohen's d effect size between two groups\"\"\"\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        if n1 < 2 or n2 < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        var1 = np.var(group1, ddof=1)\n",
    "        var2 = np.var(group2, ddof=1)\n",
    "        pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "        \n",
    "        if pooled_se == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return abs((np.mean(group1) - np.mean(group2))) / pooled_se\n",
    "\n",
    "    def _run_significance_tests(self, attention_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run statistical tests on attention weight differences\"\"\"\n",
    "        neutral_mask = attention_data.Val_class == \"neutralize\"\n",
    "        neutral_weights = attention_data[neutral_mask][\"AttentionWeight\"].values\n",
    "        \n",
    "        results = []\n",
    "        classes = attention_data.Val_class.unique()\n",
    "        \n",
    "        for val_class in classes:\n",
    "            if val_class == \"neutralize\":\n",
    "                continue\n",
    "            \n",
    "            class_mask = attention_data.Val_class == val_class\n",
    "            class_weights = attention_data[class_mask][\"AttentionWeight\"].values\n",
    "            \n",
    "            t_stat, p_val = stats.ttest_ind(class_weights, neutral_weights)\n",
    "            \n",
    "            results.append({\n",
    "                \"valence_class\": val_class,\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_val\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df[\"adjusted_p\"] = multipletests(results_df[\"p_value\"], method=\"fdr_bh\")[1]\n",
    "        \n",
    "        for col in [\"p_value\", \"adjusted_p\"]:\n",
    "            results_df[col] = results_df[col].round(4)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    def _analyze_attention_shifts(self, attention_data: pd.DataFrame, baseline: pd.Series, top_n: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"Identify words with largest attention weight changes\"\"\"\n",
    "        shifts = (attention_data\n",
    "                 .groupby(['Word', 'Val_class'], observed=True)['AttentionWeight']\n",
    "                 .mean()\n",
    "                 .unstack()\n",
    "                 .fillna(0))\n",
    "\n",
    "        neutral_mask = ~shifts['neutralize'].isna()\n",
    "        valid_words = shifts[neutral_mask].index\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        for val_class in shifts.columns:\n",
    "            if val_class == 'neutralize':\n",
    "                continue\n",
    "            \n",
    "            shift_data = pd.DataFrame({\n",
    "                'Word': valid_words,\n",
    "                'Neutral_Weight': shifts.loc[valid_words, 'neutralize'],\n",
    "                'Shifted_Weight': shifts.loc[valid_words, val_class],\n",
    "                'Valence_Class': val_class\n",
    "            })\n",
    "            \n",
    "            shift_data['Absolute_Shift'] = abs(shift_data['Shifted_Weight'] - shift_data['Neutral_Weight'])\n",
    "            shift_data['Percentage_Change'] = (\n",
    "                (shift_data['Shifted_Weight'] - shift_data['Neutral_Weight']) / \n",
    "                shift_data['Neutral_Weight'] * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            top_shifts = (shift_data\n",
    "                         .nlargest(top_n, 'Absolute_Shift')\n",
    "                         .reset_index(drop=True))\n",
    "            \n",
    "            result_dfs.append(top_shifts)\n",
    "        \n",
    "        final_shifts = pd.concat(result_dfs, ignore_index=True)\n",
    "        \n",
    "        formatted_shifts = (final_shifts\n",
    "            .assign(\n",
    "                Neutral_Weight=lambda x: x['Neutral_Weight'].round(4),\n",
    "                Shifted_Weight=lambda x: x['Shifted_Weight'].round(4),\n",
    "                Absolute_Shift=lambda x: x['Absolute_Shift'].round(4)\n",
    "            )\n",
    "            .sort_values(['Valence_Class', 'Absolute_Shift'], ascending=[True, False])\n",
    "            .reset_index(drop=True))\n",
    "        \n",
    "        formatted_shifts['Rank'] = (formatted_shifts\n",
    "            .groupby('Valence_Class')\n",
    "            .cumcount() + 1)\n",
    "        \n",
    "        return formatted_shifts[[\n",
    "            'Rank',\n",
    "            'Valence_Class', \n",
    "            'Word',\n",
    "            'Neutral_Weight',\n",
    "            'Shifted_Weight', \n",
    "            'Absolute_Shift',\n",
    "            'Percentage_Change'\n",
    "        ]]\n",
    "\n",
    "    def plot_prediction_distribution(self, diagnosis: str, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot distribution of predictions for a specific diagnosis\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for valence, df in self.diagnosis_dfs.items():\n",
    "            if diagnosis not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            numeric_values = pd.to_numeric(df[diagnosis], errors='coerce')\n",
    "            if numeric_values.notna().any():\n",
    "                sns.kdeplot(data=numeric_values.dropna(), label=valence)\n",
    "\n",
    "        plt.title(f'Prediction Distribution for {diagnosis}')\n",
    "        plt.xlabel('Prediction Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def get_distribution_statistics(self, diagnosis: str) -> pd.DataFrame:\n",
    "        \"\"\"Calculate statistical summary for prediction distributions\"\"\"\n",
    "        stats_data = []\n",
    "\n",
    "        for valence, df in self.diagnosis_dfs.items():\n",
    "            if diagnosis not in df.columns:\n",
    "                continue\n",
    "\n",
    "            numeric_data = pd.to_numeric(df[diagnosis], errors='coerce')\n",
    "            df_cleaned = numeric_data.dropna()\n",
    "            \n",
    "            if len(df_cleaned) > 0:\n",
    "                stats_dict = {\n",
    "                    'valence': valence,\n",
    "                    'count': len(df_cleaned),\n",
    "                    'mean': df_cleaned.mean(),\n",
    "                    'std': df_cleaned.std(),\n",
    "                    'min': df_cleaned.min(),\n",
    "                    'max': df_cleaned.max(),\n",
    "                    'median': df_cleaned.median()\n",
    "                }\n",
    "                stats_data.append(stats_dict)\n",
    "\n",
    "        return pd.DataFrame(stats_data) if stats_data else pd.DataFrame()\n",
    "\n",
    "    def plot_attention_heatmap(self, top_n: int = 20, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot heatmap of attention weight changes\"\"\"\n",
    "        if 'neutralize' not in self.attention_dfs:\n",
    "            return\n",
    "\n",
    "        base_attention = self.attention_dfs['neutralize']\n",
    "        top_words = base_attention.groupby('Word')['AttentionWeight'].mean().nlargest(top_n).index\n",
    "\n",
    "        attention_matrix = []\n",
    "        valences = []\n",
    "\n",
    "        for valence, df in self.attention_dfs.items():\n",
    "            word_attention = df[df['Word'].isin(top_words)].groupby('Word')['AttentionWeight'].mean()\n",
    "            attention_matrix.append(word_attention)\n",
    "            valences.append(valence)\n",
    "\n",
    "        attention_df = pd.DataFrame(attention_matrix, index=valences, columns=top_words)\n",
    "\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(attention_df, cmap='RdBu_r', center=0, annot=True, fmt='.2f')\n",
    "        plt.title('Attention Weight Changes Across Valences')\n",
    "        plt.xlabel('Words')\n",
    "        plt.ylabel('Valence')\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def get_available_diagnoses(self) -> List[str]:\n",
    "        \"\"\"Get list of available diagnoses\"\"\"\n",
    "        diagnoses = set()\n",
    "        for df in self.diagnosis_dfs.values():\n",
    "            exclude_cols = {'Valence', 'Val_class', 'Word', 'AttentionWeight'}\n",
    "            diagnoses.update(set(df.columns) - exclude_cols)\n",
    "        return list(diagnoses)\n",
    "\n",
    "    def generate_all_visualizations(self, save_dir: str, diagnoses: Optional[List[str]] = None):\n",
    "        \"\"\"Generate all visualizations\"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if diagnoses is None:\n",
    "            diagnoses = self.get_available_diagnoses()\n",
    "\n",
    "        for diagnosis in diagnoses:\n",
    "            diagnosis_path = save_path / f\"prediction_dist_{diagnosis}.png\"\n",
    "            self.plot_prediction_distribution(diagnosis, str(diagnosis_path))\n",
    "\n",
    "        heatmap_path = save_path / \"attention_heatmap.png\"\n",
    "        self.plot_attention_heatmap(save_path=str(heatmap_path))\n",
    "\n",
    "        self._generate_summary_report(save_path / \"analysis_report.md\")\n",
    "\n",
    "    def _generate_summary_report(self, save_path: Path):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        analysis = self.analyze_attention_patterns()\n",
    "        \n",
    "        with open(save_path, \"w\") as f:\n",
    "            f.write(\"# Attention Analysis Report\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Word Type Effects\\n\")\n",
    "            if not analysis.val_type_effects.empty:\n",
    "                f.write(analysis.val_type_effects.to_markdown() + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No valid word type effects found.\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Word Class Effects\\n\")\n",
    "            if not analysis.val_class_effects.empty:\n",
    "                f.write(analysis.val_class_effects.to_markdown() + \"\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No valid word class effects found.\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Distribution Statistics by Diagnosis\\n\")\n",
    "            for diagnosis in self.get_available_diagnoses():\n",
    "                stats_df = self.get_distribution_statistics(diagnosis)\n",
    "                if not stats_df.empty:\n",
    "                    f.write(f\"\\n### {diagnosis}\\n\")\n",
    "                    f.write(stats_df.to_markdown() + \"\\n\")\n",
    "\n",
    "    def analyze_full_dataset(self, output_dir: str):\n",
    "        \"\"\"Perform comprehensive analysis of the entire dataset\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.generate_all_visualizations(str(output_path / \"visualizations\"))\n",
    "        self._generate_summary_report(output_path / \"full_analysis_report.md\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    analyzer = ResultsAnalyzer(\"result\")\n",
    "    analyzer.analyze_full_dataset(\"OUTPUT\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
